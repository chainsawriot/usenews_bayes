
---
title             : "Bayesian multilevel modeling and its application in comparative communication research"
shorttitle        : "BAYESIAN MULTILEVEL REGRESSION"

author: 
  - name          : "Chung-hong Chan"
    affiliation   : "1"
    corresponding : yes
    address       : "A5, 6 (section A), 68159 Mannheim, Germany"
    email         : "chung-hong.chan@mzes.uni-mannheim.de"
  - name          : "Adrian Rauschfleisch"
    affiliation   : "2"    

affiliation:
  - id            : "1"
    institution   : "Mannheimer Zentrum für Europäische Sozialforschung, Universität Mannheim, Germany"
  - id            : "2"
    institution   : "National Taiwan University, Taiwan"
    
authornote: |
  The authors would like to thank professor Kasper Welbers (Vrije Universiteit Amsterdam) for his advice on the development of the Dutch dictionary; Dr Junior Yuner Zhu (City University of Hong Kong) for sharing details about her study.  The authors report there are no competing interests to declare.
  Source code and data are available at (redacted). Prereg: https://osf.io/2h4w8/

abstract: |
  Comparative approaches are frequently used in communication research, especially journalism studies. The purpose of this paper is to argue that Bayesian multilevel regression is a better option for analyzing comparative data. We argue that it is the only approach that can simultaneously account for non-atomicity (nested nature) and non-stochasticity (non-random sampling) of comparative data. Using the openly available *Worlds of Journalism Study* and *useNews* datasets and the R package *brms*, we demonstrate how to apply the Bayesian approach for the analysis of comparative data. We address the common challenges when using the Bayesian approach and highlight the advantages of posterior predictive checks for modeling checking.
keywords          : "Bayesian inference, multilevel model, comparative communication research, ecological effect"
wordcount         : "8964"

bibliography      : ["manuscript.bib"]

floatsintext      : yes
figurelist        : no
tablelist         : no
figsintext        : yes
footnotelist      : no
linenumbers       : no
mask              : yes
draft             : no

documentclass     : "apa6"
classoption       : "man"
output:
 papaja::apa6_pdf:
   latex_engine: xelatex
---

```{r setup, include = FALSE}
library("papaja")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

```{r, echo = FALSE}
pts_tab <- readRDS("pts_tab.RDS")
require(brms)
import_brms <- readRDS("import_brms.RDS")
```

<!-- *"The comparative approach attempts to reach conclusions beyond single cases and explains differences and similarities between objects of analysis against the backdrop of their contextual conditions."* @esser:2017:CRM -->

Comparative approaches are frequently used in communication research, especially in journalism studies. Journalism studies are using the comparative approach [@hanusch:2019:C] to model journalistic culture [e.g. @esser:2013:C], news value [e.g. @burggraaff:2017:T;@wilke:2012], news flow [e.g. @wu:2000:SDI;@grasland:2019:I], among others. Around 40% of recent  comparative studies in journalism research are comparative content analysis [@hanusch:2019:C], which include news articles from various outlets and usually also from various countries. Then the included news articles are coded either manually or automatically. Another 10% of recent comparative studies in journalism research are surveys such as the Worlds of Journalism Study (WJS) [@hanusch:2017:CJC]. Although this article uses examples from journalism studies, the comparative approach is not bound to journalism studies and has been widely deployed in various subfields of communication. Comparative communication research is not exclusive to comparison across countries. Based on @de2017comparative’s classification, one can do comparisons across times, media, and other units [e.g. language regions within a multilingual country, see @vogler:2021:TNM]. In social media studies, for instance, it is common to compare content across sources [e.g. @zhao2019appealing]. The methodological issues discussed in this paper apply also to those cases.

Although some comparative studies are purely descriptive, most of these studies are aimed to study how contextual conditions such as characteristics of media outlets (e.g. political orientation, online versus offline) or their countries (e.g. post-communist country) influence the behaviors of journalists or the content of news articles . For example, some hypotheses are "the coverage of foreign countries by the news is primarily determined by geographical proximity and the status of the covered country" [@wilke:2012, p.306] and "the degree of opinion-orientation will be highest in newspapers from Polarized Mediterranean systems and lowest in those from Anglo-American systems." [@esser:2013:C] Employing the language from epidemiology [@susser:1994], the effect on media content and journalistic behaviors in these hypotheses is assumed to be ecological. It assumes a macro contextual factor (e.g., newspapers from Polarized Mediterranean systems) is associated with an outcome at the micro-level (e.g., degree of opinion-orientation), namely, the journalist-level or article-level. 

This study of ecological effects on individual behaviors has a long tradition in social sciences. A famous example was Émile Durkheim's investigation on the ecological factors associated with suicide [@durkheim1897suicide]. Durkheim found a higher suicide rate among citizens from Scandinavian countries than those from other European countries. Using today’s standard, Durkheim's analysis is descriptive and this descriptive approach for studying ecological effect is indeed still useful for hypothesis generation in comparative research. An example of hypothesis-generating comparative research is <!-- redacted for ijoc  -->AUTHOR1 <!-- @chan2020combining -->, in which the sentiment profiles of terrorism coverage from Muslim- and Christian-majority countries were visualized. Hypothesis-generating comparative study, however, is rare. Most studies have a confirmatory outlook. As we see from the example hypotheses above, most of these studies propose hypotheses to test for ecological effects. In this context usually traditional (frequentist) hypothesis testing approaches were used: @esser:2013:C lumped multiple outlets from the same country together and then used univariate analysis of variance to test for the differences in the proportion of opinion-orienting articles across countries. In @wu:2000:SDI, multiple stepwise regression was used. These approaches violate the underlying assumptions. It highlights the fact that comparative research introduces a feature that researchers usually overlook: media contents are clustered in a multilevel structure. A news article is nested within its media outlet and its media outlet is in turn nested within its country. Such data structure brings two problems: non-atomicity and non-stochasticity. And specifically, we propose two solutions: multilevel modeling and Bayesian approach.

The goal of this paper is twofold. We use examples from journalism studies as the most critical cases in communication science to highlight the strength of multilevel models and the advantages of Bayesian models. In our paper, we first discuss these two issues before we suggest Bayesian multilevel regression as the best possible approach for comparative research in communication science and especially in journalism studies. Using the openly available WJS and *useNews* datasets and the R package *brms*, we then demonstrate how to apply a Bayesian approach for the analysis of comparative data.

## Multilevel models and non-atomicity

In comparative research, it is easy to incorrectly assume that macro-level independent variables (e.g. Anglo-American systems) could be analyzed at the same level as the micro-level dependent variable (e.g. the degree of opinion-orientation). The manifestation of this incorrect assumption is to enter macro-level variables as independent variables in multiple regression analysis and regress them against a micro-level dependent variable. Suppose one wants to study how the democratic performance of a country affects journalists’ perceived professional autonomy using the Worlds of Journalism data [Wave 1: @reich:2013:DJP; Wave 2: @hamada:2021:DJA]. The relationship can be expressed in the following regression equation: let $y_{i}$ be the dependent variable at the micro-level (journalist-level: perceived professional autonomy) and $x_{i}$ be the independent variable at the macro-level (country-level: democratic performance). Suppose there are $m$ journalists  where $i = 1,2,\ldots,m$.

\begin{align}
  y_{i} = \beta_{0} + \beta_{1} x_{i} + \epsilon_{i}
\end{align}

This method is the so-called "disaggregation approach". From an epistemic standpoint, searching for ecological effect by studying the value of slope ($\beta_{1}$) leads to the so-called atomistic fallacy [@hox2017multilevel]. From a statistical standpoint, this approach violates the underlying independence assumption. Almost all frequentist statistical tests assume that the observations are independent of each other: $y_{i}$ given $x_{i}$ are independently identically distributed (i.i.d.). Journalists from the same country are hardly i.i.d.: they are subjected to being governed by the same government and exposed to the same journalistic culture. Also, the way communication researchers survey the journalists resembles (nonrandom) cluster sampling rather than pure random sampling: one usually starts from a specific country and then surveys journalists from it. The choice of  countries is almost always not randomly selected. <!-- [^1] --> This sampling approach makes the independence assumption even more fragile.

<!-- [^1]: If countries were randomly selected, we shall see more African countries in comparative works. It is because 1/4, or 54 out of 195, UN countries are in Africa. -->

Previous simulation studies have shown that ignoring this dependence can lead to false-positive associations [e.g. @clarke:2008:W;@chen:2012:IIL]. The problem is more severe when the dependent variable is a discrete binary variable [@clarke:2008:W]. <!-- [^2] -->

<!-- [^2]: The non-atomicity argument has been in standard statistics textbooks for 40+ years and it might look repetitious. We believe it is still necessary, not least because multilevel modeling --unlike what @mcelreath2020statistical [p. 356] said-- is still not the default regression approach in communication research for hierarchical data and there are many justifications for using the disaggregation approach, e.g. an example in @zhu:2020:S [footnote 5]. <\!-- Even a multilevel modeling textbook advocates these justifications [@bickel2007multilevel]. -\-> See also @nezlek:2008:IMM for a criticism of these justifications.  -->

There are two solutions to this. The first solution is to aggregate the micro-level dependent variable across a macro-level independent variable. Suppose there are $n$ countries in the above example. One can aggregate the average level of $x$ as $z_{k}$ for the $k$-th country, where $k=i,\ldots,n$. Then, we can do a regression with a regression equation like so:

\begin{equation}
  \label{eq:2}
  z_{k} = \beta_{0} + \beta_{1} x_{k} + \epsilon_{k}
\end{equation}

Using this aggregation method, the unit of analysis effectively switches from journalist to country. This method is useful when $x$ is the only independent variable. It is not technically committing the atomistic fallacy, when one uses the value of slope as the evidence for an ecological effect. However, this method still has important drawbacks. First, it cannot be used for data with more than two levels. In those cases, the lower-level predictors were lumped together and in effect, assumed to be homogeneous and discarded. Second, this method discards a massive amount of information. It is better for analysis of a dependent micro-level variable with a reasonable aggregation function (e.g. counting a binary variable). For numerical variables, aggregation functions such as taking a mean or median cannot capture the spread of the micro-level variable [@bryk:1988:H]. Also, the effective sample size is reduced from the number of journalists ($m$) to the number of countries ($n$). Third, if one specifies hypotheses at the journalist-level but bases one’s conclusion on the aggregated analysis, this is a risk of interpreting the macro-level association wrongly at the micro-level, otherwise known as ecological fallacy. Nonetheless, this aggregation method, although inflexible, is still useful when the number of groups (e.g. $n$) is large. It is also useful to collapse micro-levels (e.g. article-level) that are not useful in answering one’s research questions.

Another solution is to use the multilevel model (linear mixed model, or hierarchical model). In a multilevel model, the effect on the micro-level dependent variable ($y$) is modeled with equations at different levels. Using the above example, $y_{ik}$ denotes the perceived professional autonomy of the $i$-th journalist in the $j$-th country; $x_{k}$ denotes the democratic performance of the $k$-th country. 

\begin{align}
  y_{ik} &= \beta_{0} + \epsilon_{ik} \\
  \beta_{0} &= \gamma_{00} + \gamma_{01} x_{k} + \mu_{0k}
\end{align}

In these equations, $\gamma_{00}$ is the average slope, while $\mu_{0k}$ is group-dependent deviations of the slope from the average. It is usually set as having a normal distribution with a variance $\tau_{00}$, i.e. $\mu_{0k} \sim \mathcal{N}(0, \tau_{00})$. Instead of a single value, the regression coefficient $\gamma_{00}$ is assumed to be a distribution of values depending on a macro-level group. It addresses the problem of clustering of articles by macro-level variables. This model is called the varying-intercept model and is used frequently in social science research. We can then study the magnitude of $\gamma_{01}$ to determine the ecological effect.

The benefit of using multilevel modeling lies in its flexibility in handling multi-level data. Suppose we also want to consider the clustering of journalists around $o$ different media organizations in the above example and $j$-th media media organization of the $i$-th journalist, where $j = 1,\ldots, o$ and if such data were available. [^WJSOUTLET] The multilevel regression equations are rewritten as:

\begin{align}
y_{ijk} &= \pi_{0} + \epsilon_{ijk} \\
\pi_{0} &= \beta_{00} + \mu_{0j} \\
\beta_{00} &= \gamma_{000} + \mu_{00k} + \gamma_{001} x_{k}
\end{align}

[^WJSOUTLET]: The data on media organizations are not available in the 2nd wave of the WJS dataset.

This flexibility is demonstrated in the study by @rinke:2016:ISB. He studied the likelihood of opinion justification in 1559 utterances nested in 329 news items, which were in turn nested in 101 news broadcasts. Multilevel logistic regression was used to model the natural three-level hierarchy of his data.

Another way to look at the flexibility of multilevel modeling is the ability to vary other parameters in the regression equation as well. Suppose in another situation, a researcher is interested in analyzing the relationship between experience ($XP$) and perceived professional autonomy. Instead of assuming the effect to be uniform across countries, like in the following equation:

\begin{align}
y_{i} &= \beta_{0} + \beta_{1} XP_i + \epsilon_{i}
\end{align}

One can also assume the slope ($\beta_{1}$) to be a distribution that is governed by countries, i.e.

\begin{align}
y_{ik} &= \beta_{0} + \beta_{1} XP_{ik} + \epsilon_{ik} \\
\beta_{1} &= \gamma_{10} + \mu_{1k}
\end{align}

This model is called the varying slope model and is useful to establish robust estimation of effect (in this case $\gamma_{10}$) across all countries. In this paper, we will not go into detail of this kind of model. For an example of its application in comparative communication research, see @barnidge:2018:SMS. 

In sum, multilevel modeling is the way to go for analyzing non-atomic data from comparative research. For surveys using probabilistic sampling, the conventional frequentist approach (maximum likelihood estimation, MLE) might still be valid and is available in most statistical packages [see an SPSS tutorial for communication researchers by @hayes:2006:PMM]. However, @stegmueller:2013:HMC demonstrates that MLE is associated with shrinkage (reduction of standard error, i.e. more false positives) and the shrinkage is more severe when the number of macro-level units (e.g. countries) is small. @stegmueller:2013:HMC proposes to use Bayesian analysis as a robust alternative [see counterarguments from @elff:2020:MAF]. Although less restrictive methods such as restricted maximum likelihood (REML) have been demonstrated to remediate the shrinkage issue of MLE [@elff:2020:MAF], we still agree with Stegmueller’s proposal for theoretical reasons. And for studies not using probabilistic sampling, we argue in the next section that the Bayesian approach is a better analytical approach.[@western:1994:BIC].

## Bayesian models and non-stochasticity

*"If Czech history **could** be repeated, we should of course find it desirable to test the other possibility each time and compare the results. Without such an experiment, all considerations of this kind remain a game of hypotheses."* [@kundera2020unbearable, emphasis added]

Before diving into our theoretical reasoning, it is important to revisit what frequentist inference, the current default but often misunderstood [@rinke:2018:PMA] mode of inference in communication science, is. Under the frequentist framework, each experiment is assumed to be one of infinite independent, **repeatable** experiments on randomly drawn samples from a population. Random experiments are assumed to be repeated arbitrarily often. Based on this assumption and with just one experiment from the current study, we make an estimation about the population. The discrepancy between the estimation from that one experiment and the actual value of the population is due to sampling error alone, i.e. which subjects were randomly sampled from the population. Randomized surveys, for example, are assumed to be repeatable through repeated random sampling of the population. Suppose we replicated the same survey 100 times and we would obtain a slightly different sample every time. We then calculated the 90\% confidence interval of the mean for each of these 100 surveys. We should anticipate that roughly 90 out of these 100 confidence intervals would include the true mean of the population. We cannot say for sure exactly 90 out of these 100 confidence intervals would include the true mean of the population because repeated random sampling is indeed random and the process is **stochastic**. However, we can say 90 is more probable than 0 or 100.

Most of the comparative content analytic studies, unlike randomized surveys such as WJS, often collect all available content data from a bunch of selected media outlets. In contrast to cluster sampling where media outlets are sampled randomly from a sampling frame of all media outlets, these studies are collecting the entire population of observations from some nonprobailistically selected media outlets. In these census-like situations, there is no way to get more data unless the scope of these studies is changed [@berk:1995:SIA]. It is especially true for modern large-N studies using automated content analytic techniques. @burggraaff:2017:T, for instance, "collected **all** available news items from **a selection of major Dutch news outlets**, both online and print" (p. 6, emphasis added) and that amounted to 762,095 articles from 9 outlets in the period of 2014--2015. In that study, one could only get more data by including more media outlets or widening the time window. Unlike a repeatable survey like WJS, one can always randomly select more journalists from the sampling frame of all journalists in the respective country.

Therefore, these modern comparative content analytic studies are not repeatable and thus they generate non-stochastic data. It could be argued that data from these comparative content analytic studies are fundamentally irrelevant for frequentist inference [@western:1994:BIC]. Confidence intervals generated do not have the same meaning as those from repeatable studies. According to @western:1994:BIC [p. 413], these values from non-stochastic data "lack meaning even as abstract propositions". [^PVALUES]

[^PVALUES]: In this article, we will not go into detail of the null hypothesis statistical testing (NHST) and p-values, except in the Online Appendix. It is because for studies with a very large N, which comparative studies usually are, NHST is guaranteed to give positive results due to the so-called “Too Big To Fail” problem [see @lin:2013:RCT]. Also, researchers are usually not only interested in whether there is or isn’t an effect. Instead, we are interested in how large the magnitude and in what direction the effect is. Therefore, we will focus only on the point and interval estimations from various regression models.

A common counterargument to this is the classic one from @deming:1941:ICS [p. 45], who suggested that "[a]s a basis for scientific generalizations and decisions for action, a census is only a sample." This notion assumes a census of a population can be used to make inference on a theoretical device called *superpopulation* which "theoretically could exist, may have existed, or may exist in the future" [@gibbs:2015:IU, p.3]. In other words, a finite census-as-a-sample census is assumed to be a “representative sample” of an infinite superpopulation. This counterargument could be useful but unlike a regular random sample (cluster sample included), whose representativeness can be assessed, we can never assess the representativeness of the census-like data with respect to the theoretical superpopulation. Echoing the quote at the beginning of this paragraph, there is no way to tell if the current Czech history is representative of all possible Czech histories in the multiple parallel universes.

Instead of invoking the theoretical device of superpopulation, we follow the arguments from @western:1994:BIC and @stegmueller:2013:HMC for comparative studies: Bayesian inference should be used for analyzing data from comparative research in our field. Choosing a Bayesian approach also solves the problem of misinterpretation of confidence intervals [@rinke:2018:PMA]. Switching to a Bayesian approach addresses this issue as Bayesian "credible intervals support an interpretation of probability in terms of plausibility" [@morey:2015, p.120]. <!-- However, for Bayesian models so-called priors have to be chosen by researchers. -->

Before we move on to the next section, it is important to point out that **non-stochastic data is not an essential condition for applying the Bayesian approach**. The same approach is equally applicable to both non-stochastic and stochastic comparative datasets. Therefore, one can analyze both randomized comparative surveys (such as WJS) and comparative content analytic studies with the same Bayesian analytic approach. Besides these more abstract conceptual implications, choosing a Bayesian approach has some practical advantages, as we will later illustrate with the posterior predictive checks (PPCs), which can only be calculated if a Bayesian approach is used.

## Bayesian analysis as alternative

What is the probability for this paper being accepted by *IJOC*? Under the frequentist framework (and if *IJOC* were accepting papers stochastically), we can only find this out by repeatedly submitting this paper to *IJOC*, say for 100 times, and then count the frequency of acceptance in these repeated submissions. It is indeed impractical as well as inhumane to the editorial team of *IJOC*. Instead, we assert before submission that this paper has a probability of 24% for being accepted. That is the published acceptance rate of a similar journal. After this paper is submitted and is not desk rejected, the probability might be around 24% to 30%. After a month of waiting and our confidence is shaken a little, the probability might decrease to 10% to 20%. After the paper is mixed reviewed by three reviewers and an R&R is invited, the probability might increase to 40% to 60%. If you see this paper on *IJOC*'s website, then the probability is beyond doubt 100% (or 0% if we submit this paper again).

Without repeated experiment, these probabilities quantify our certainty on how likely this paper is being accepted, given the current available data . We revise our old beliefs (or **prior**, $p(\theta)$) with the new data ($X$) and form our revised belief (or **posterior**, $p(\theta|X)$). This can be summarized in the following equation [@gelman2020bayesian]:

\begin{align}
        P(\theta | X) \propto  P(\theta) P(X| \theta)
\end{align}
 
The $P(X| \theta)$ part is called likelihood function. In the *IJOC* example, the likelihood function is based on rough rules from our experience and thus is not systematic. In actual analysis, we need to derive such a likelihood function based on the available data using methods such as Markov Chain Monte Carlo (MCMC). Nonetheless, the above equation indicates that there are only three ingredients in any Bayesian analysis: 1) data ($X$), which require no elaboration, 2) a method to derive the likelihood function $P(X | \theta)$ from the data, and 3) prior, $P(\theta)$. 

R interfaces to Stan (the probabilistic programming language for conducting MCMC), such as *brms* [@burkner2017advanced], can enable the derivation of the likelihood function (Part 2). But still, it is important to be mindful that Bayesian analysis is much more computational intensive than methods such as MLE. Our benchmark suggests that Bayesian analysis needs at least 100 times more running time than MLE.

Part 3 (Prior) is arguably the most controversial part of Bayesian analysis. In the *IJOC* example, we can select a reasonable prior (or **informative** prior) of 24% from published information. Finding previous studies for an informative prior is the logical first thing to do. @keating:2019:W show that one in every seven communication research papers published in major communication journals was a form of replication attempt. For these one seventh of communication research, there should be previous studies available to base one’s informative prior non-controversially.

For the other six sevenths, one might not have any information to set an informative prior. One option is to consult experts or to make an educated guess. Expert elicitation is a way to probe how the experts in the field think about the hypotheses. A standardized protocol for expert elicitation is available [@hanea:2017:IDE] and there are many software tools available to facilitate the process [^experttools].

[^experttools]: Some examples are the web-based MATCH Uncertainty Elicitation Tool (http://optics.eee.nottingham.ac.uk/match/uncertainty.php) and the R package SHELF.

But one person's expert opinion could be another person's wishful thinking. And this perceived subjectivity of specifying priors by experts’ judgment attracts wide-spread criticism from both statisticians [e.g. @efron1986isn] and social scientists [e.g. @elff:2020:MAF].

Undoubtedly, setting prior is consequential to the analysis. But the influence from priors is greatly weakened, when the data is getting bigger. Other than expert elicitation, another non-controversial way to specify priors --- at least in our opinion --- is to use a weakly informative prior [@lemoine:2019:M]. In this way, one specifies only the possible range of the posterior<!-- [^4] -->. Some so-called "default weakly informative priors", e.g. $\mathcal{t}(1, 0, 0.25)$, have been suggested for typical regression models [@gelman:2008].

<!-- [^4]: One could also use a completely *noninformative* prior such as $\mathcal{U}(-\infty, \infty)$. It is, however, no longer advised. -->

## Bayesian communication research

<!-- *"There is a general challenge to a prescription of more widespread use of Bayesian methods for multilevel modeling, and that is that such methods require statistical expertise beyond that of most applied social science researchers, as well as specialist software (or software with which such researchers are unfamiliar)."* @bryan:2015:MMC [p. 19] -->

Bayesian analysis is still a minority statistical method in social sciences. It is reasonable to say that Bayesian analysis is extremely rare in our field. Although some communication researchers adopted the method (e.g. AUTHOR2, AUTHOR3) <!-- [e.g. @kovic:2017:B;@chan2020high] -->, these studies are not comparative. As far as we know, the only available comparative communication research that Bayesian multilevel regression was used for studying ecological effect are @leeuw:2020:AAT and @heidenreich:2022:DE. We believe comparative communication research fits the use case of Bayesian multilevel regression analysis.This paper demonstrates how to do the analysis using the R package *brms* [@burkner2017advanced] (see the source code in the Online Appendix https://osf.io/2h4w8/?view_only=be14dc637b8e4fd1a26ac5c64682b6d9 ). As the interface of *brms* is almost the same as *lme4* [@bates:2015:FLM, another R package for fitting multilevel models using MLE], *lme4* users might find *brms* extremely familiar. <!-- The above-quoted challenge about the prescription of more widespread use of Bayesian methods for multilevel modeling is no longer an issue.  -->

The two examples below show how the Bayesian multilevel regression analysis can be deployed to the common archetypes of comparative journalism studies. The first example is an analysis of a comparative, stochastic survey dataset (WJS). This example was chosen to highlight the multilevel (non-atomicity) aspect and represent the situation of replication studies. The second example is an analysis of a comparative, non-stochastic content analytic dataset. This example was chosen to highlight the necessity of using the Bayesian approach to analyze non-stochastic comparative data and represent the situation of studies where noninformative priors were needed.

# Example 1: The Worlds of Journalism study

In this example, the Bayesian approach is applied to a stochastic dataset from a randomized survey. The starting point of this example is the recent study by @hamada:2021:DJA. Using the second wave data from WJS (2012-2016), the study seeks to study this hypothesis (the H2a in the paper): *The greater the level of democracy in a country, the more perceived professional autonomy journalists enjoy*. To rephrase this, the level of democracy in a country has an ecological effect on the journalists’ perceived professional autonomy. 

It is important to point out that the study is a replication. The same hypothesis has been studied in the earlier study by @reich:2013:DJP, which uses the first wave data from WJS and has a remarkably similar title to that of @hamada:2021:DJA. The earlier study has studied this hypothesis (the H4): *Journalists’ perceived professional autonomy is positively associated with democratic performance and press freedom, and it is negatively related to political parallelism and state intervention.* The operationalizations of both professional autonomy (based on two questions in WJS) and level of democracy (based on the Economist Intelligence Unit's Index of Democracy) are the same in the two studies. It is important to recite the operationalization of professional autonomy here. The two questions in the survey are: (1) “Thinking of your work overall, how much freedom do you personally have in selecting news stories you work on?” (2) “How much freedom do you personally have in deciding which aspects of a story should be emphasized?” The possible answers ranged from 5 = “complete freedom” to 1 = “no freedom at all”. The two answers were averaged. We will come back to this operationalization later in this article.

Interestingly, the analytical approaches are also similar in the two studies. The study by @reich:2013:DJP contains multiple regression models on how journalist-level characters predict perceived professional autonomy. However, for country-level predictors such as Index of Democracy, @reich:2013:DJP apply an aggregated approach due to "methodological considerations" of "includ[ing] substantive country-level predictors in an OLS regression" (p. 146). Effectively, the analysis boils down to aggregating the perceived professional autonomy of all journalists into mean values according to baskets of Index of Democracy and then comparing those mean values by ANOVA. The subsequent study by @hamada:2021:DJA applies the same aggregated approach by studying the bivariate correlation between the mean perceived professional autonomy of all journalists in a country and the Index of Democracy of a country.

## Bayesian multilevel analysis of WJS

Let's assume we were in the shoes of @hamada:2021:DJA and wanted to replicate the study by @reich:2013:DJP with the second wave WJS data. Bayesian multilevel regression is a better approach. 

First, it can take advantage of the hierarchical structure of the data and correctly estimate the contextual effect of democratic performance on the journalist-level perceived professional autonomy. It allows for adjustment of other journalist-level predictors that are known to influence perceived professional autonomy, e.g. rank ($RANK$), experience ($XP$), gender ($GEN$), & having a university degree ($UNIV$). It also makes it possible to adjust for the possible confounding effects of other country-level variables. For example, GDP per capita (according to the World Bank) is moderately correlated with the Index of Democracy (r = `r cor(pts_tab$eiu, pts_tab$gdppc, use = "complete")`). The correlation between Index of Democracy ($DEMO$) and perceived professional autonomy ($PPA$) found by @reich:2013:DJP could be spurious and the GDP per capita were the actual determinant of perceived professional autonomy. We can adjust for the effect of GDP per capita by entering it also as an independent variable. The data can also provide such variance because there are countries with high GDP per capita but with low democracy (e.g. UAE and Qatar) and vice versa (e.g. Botswana and India).

## Priors

The idea of choosing priors is to select a probable probability distribution for each of unknown parameters in a model. Usually, the first step for choosing priors is to review previous studies and look for possible values to be used as our informative priors. Incorporating of prior information from @reich:2013:DJP in this Bayesian analysis makes the intention to replicate clearer. 

@reich:2013:DJP suggest that the relationship between Index of Democracy and perceived professional autonomy is in "J shape", with the average perceived professional autonomy of journalists in authoritarian regimes (the lowest end of democratic performance) being higher than those in hybrid regimes (3.92 vs 3.65). We can incorporate this prior information into our model by modeling a cubic relationship between Index of Democracy and perceived professional autonomy. Like the general procedure of conducting polynomial regression, we also create a parsimonious model assuming only a linear relationship to study whether the cubic regression improves the model fit. In other words, whether the relationship is really in "J shape" is studied <!-- [^jshape] -->. It’s important to spell out all the equations so that we can have a better idea of all the estimands. For the parsimonious model, the regression equations are:

\begin{align}
  PPA_{ik} &= \beta_{0} +\beta_{1} XP_i + \beta_{2} RANK_i + \beta_{3} GEN_i + \beta_{4} UNIV_i + \epsilon_{ik} \\
  \beta_{0} &= \gamma_{00} + \gamma_{01} DEMO_{k} + \gamma_{02} \log{GDP_{k}} + \mu_{0k}
\end{align}

For the cubic model, the regression equations are:

\begin{align}
  PPA_{ik} &= \beta_{0} +\beta_{1} XP_i + \beta_{2} RANK_i + \beta_{3} GEN_i + \beta_{4} UNIV_i + \epsilon_{ik} \\
  \beta_{0} &= \gamma_{00} + \gamma_{010} DEMO_{k} + \gamma_{011} DEMO^{2}_{k} + \gamma_{012} DEMO^{3}_{k}+ \gamma_{02} \log{GDP_{k}} + \mu_{0k}
\end{align}

From the model 4 in @reich:2013:DJP, we know that the standardized beta coefficients for the variables rank ($\beta_{1}$) and professional experience ($\beta_{2}$) are .15 and .07 respectively. Unfortunately, the associated standard errors (or standard deviations) were not reported but only asterisks representing statistical significance (p < 0.001 for rank, p < 0.01 for professional experience). We estimated the maximum standard deviation based on the significance level. For example, the standard deviation of the standardized regression coefficient ($\sigma_{\beta'_{1}}$) of rank can be estimated by solving <!-- [^wald] -->:

\begin{align}
Pr(\frac{\beta'_{1}}{\sigma_{\beta'_{1}}}) &= \frac{0.001}{2} \sim \mathcal{N}(0, 1) \\
\frac{\beta'_{1}}{\sigma_{\beta'_{1}}} &= 3.29 \\
\frac{0.15}{\sigma_{\beta'_{1}}} &= 3.29 \\
\sigma_{\beta'_{1}} &= 0.045
\end{align}

The above information can be used as an informative prior of the current analysis. Unfortunately, the ANOVA result in relation to $\gamma_{01}$ from @reich:2013:DJP cannot be used as the prior and we need to use weakly informative priors suggested by @lemoine:2019:M. For regression coefficients, e.g. $\gamma_{01}$, we used a normal distribution of $\mathcal{N}(0, 1)$. For variance terms, e.g. $\mu_{0k}$, a student t-distribution of $\mathcal{t}(3, 0, 2.5)$ was used. All, except the prior for regression coefficients, are default priors suggested by *brms*. In practice, we suggest pre-registering the priors before the data collection. As this is a secondary data analysis, we cannot do that.

In order to make the result from this replication comparable to that of @reich:2013:DJP, we also calculate the standardized regression coefficients, i.e. all variables are mean-centered.

### Other parameters

Other parameters such as *adapt_delta* control how the MCMC should be performed. It is in general safe to use the default values unless MCMC shows evidence of nonconvergence. We provide a short guide on how to diagnose convergence (online appendix).

## Modeling

For this analysis, we tried 3 different approaches: 1) Disaggregation approach (ignoring the multilevel structure); 2) Multilevel regression using MLE; and 3) Bayesian multilevel regression. In the disaggregation case, the multilevel structure is ignored and the regression equations become:

\begin{align}
  PPA_{ik} &= \beta_{0} +\beta_{1} XP_i + \beta_{2} RANK_i + \beta_{3} GEN_i + \beta_{4} UNIV_i +  \beta_{5} DEMO_{k} + \beta_{6} GDP_{k} + \epsilon_{i}
\end{align}

\begin{align}
  PPA_{ik} &= \beta_{0} +\beta_{1} XP_i + \beta_{2} RANK_i + \beta_{3} GEN_i + \beta_{4} UNIV_i +  \beta_{5} DEMO_{k} + \beta_{6} GDP_{k} \\ + \beta_{7} DEMO^2_{k} + \beta_{8} DEMO^3_{k} + \epsilon_{i}
\end{align}

It is important to remind our readers that the results from the three are likely to be similar, but it is not evidence for the disaggregation approach and the MLE approach is a replacement of the Bayesian approach [@morey:2015]. The interval estimates have different meanings and can’t be directly compared. Comparing the three approaches, the disaggregation approach is the least useful because it violates the underlying independence assumption.

## Results

The Bayesian multilevel regression gave the *posterior distribution*, $P(\theta | X)$, of the regression coefficients. It is a distribution and therefore we need ways to display both the central tendency and spread of the distribution. By default, *brms* displays the mean and the 95\% High Density Interval (HDI). <!-- [^5] --> These two values represent the point and interval estimates of the regression coefficient.

<!-- [^5]: HDI is one representation of the Bayesian credible interval. There should be no "correct" choice of the level of credibility but some established conventions. We choose, like many published works [e.g. @stegmueller:2013:HMC<\!-- ; @kovic:2017:B; @chan2020high -\->], the default value of 95\%. However, there is a general concern about this credibility level as being unstable for Bayesian analysis [@kruschke2014doing]. -->

```{r}
wjs_reg <- readRDS("wjs_reg.RDS")
wjs_reg$term <- c("(Intercept)", "Experience", "Rank", "Gender (Female)", "University Degree", "Index of Democracy", "GDP per capita")
colnames(wjs_reg)[2:4] <- c("Bayesian", "MLE", "Disaggregation")
apa_table(wjs_reg, caption = "Point and interval estimates from WJS analysis using three different analytic strategies: Bayesian multilevel modeling, multilevel modeling using MLE, and Disaggregation approach")
```

By looking at the regression coefficient and the 95\% HDI of the parsimonious model (Table 1), the standardized regression coefficient for Index of Democracy is 0.24 (95\% HDI: 0.13 to 0.34). The Index of Democracy is the strongest predictor among all other predictors (Rank: 0.15, Experience: 0.08, gender: -0.06, university degree: -0.01). It supports @hamada:2021:DJA's h2a.

By comparing the point and interval estimates using different approaches, point estimates are remarkably similar. Bayesian multilevel regression gave a wider 95% HDI than confidence intervals from frequentist methods. It is like previous studies [@stegmueller:2013:HMC; @elff:2020:MAF]. While the frequentist confidence intervals and the Bayesian credible interval seem to be similar, they are from a philosophical point of view totally different and lead to different forms of inference. The Bayesian approach allows us to say that *given the observed data, the true estimate of the standardized regression coefficient has 95% probability of falling within  0.13 and 0.34*. In contrast, the frequentist confidence interval only allows us to say that *when computing a confidence interval from the same type of data in repeated studies (if possible), 95% of the confidence intervals will include the true estimate of the standardized regression coefficient*. One of the misconceptions of frequentist confidence intervals is that they can be interpreted in a Bayesian way as described above [@morey:2015]. This example illustrates that a Bayesian approach allows us to interpret the results in a more intuitive way.

### J Shape

<!-- Regarding the possible "J shape" relationship suggested by @reich:2013:DJP, we compare the model fit ($R^{2}$) of the cubic model and the parsimonious model and the cubic model provides no improvement in model fit over the parsimonious model (both: 0.194) [^loo].  -->By looking at the conditional effects plot of the cubic model (Figure \@ref(fig:fig5)), it suggests that the relationship is not "J shape" [^loo]. <!-- On average, a journalist in countries that scored a lower Index of Democracy has a lower perceived professional autonomy.  -->

```{r fig5, echo = FALSE, fig.cap = 'Conditional effects plot showing no "J shape" relationship between perceived professional autonomy and Index of Democracy.'}
require(ggplot2)
x <- readRDS("qmod_condit.RDS")
x$`eiu` + xlab("Index of Democracy (mean-centered)") + ylab("Perceived professional autonomy (mean-centered)") + theme_minimal() + scale_color_brewer(palette = "Dark2") + scale_fill_brewer(palette = "Dark2")
```

[^jshape]: Actually, the replication by @hamada:2021:DJA has shown that the relationship is not in such J shape. But we were assumed to be in the time point before such replication.

[^loo]: Another way to evaluate the model fit is to use leave-one-out cross validation. See the online appendix for the analysis.

### Posterior predictive checks

One unique feature of the Bayesian approach is posterior predictive checks (PPC) for studying how our model works. Bayesian analysts have a strong culture of model checking [e.g. @mimno:2015:P; @gelman2007comment], while many analyses – not only in communication research – end with regression tables and inferences.

A good model for theory testing should pin down the data-generating process in the real world, not just whether the model fits the data. Therefore, a good model should be able to generate simulated data that are like the observed data [@gelman2007comment]. The gist of posterior predictive check is to use the fitted model to generate some simulated data ($y_{rep}$) and compare them with the original observed data ($y$). A good model should display a similar probability distribution of $y_{rep}$ and $y$. (Figure \@ref(fig:fig3) ).

As one can see in Figure \@ref(fig:fig3), the fitted model can get the range and the peak of the original data about right. But the model can only give simulated data $y_{rep}$ with a gaussian distribution whilst the shape of the original data $y$ is not gaussian at all. The finger-like shape is a manifestation of analyzing the two ordinal variables (two Likert items) as if they were in an interval scale. With this check, it shows the original analytic scheme of both @reich:2013:DJP and @hamada:2021:DJA cannot capture the shape of the data. Also, the psychological distances between two neighboring options in a Likert scale (e.g. “complete freedom” and “a great deal of freedom”) are not the same across all pairs and one can’t assume that Likert items can be summed or averaged [@buerkner:2019:ORM].

```{r fig3, echo = FALSE, fig.cap = "Posterior predictive checks with 100 sets of simulated data based on the parsimonious model"}
readRDS("ppa_mod_ppc.RDS")
```

If this were not a replication and one didn't need to accept the previous operationalization of perceived professional autonomy, a better way to answer this research question using the WJS data is to make two modifications. First, one should use the ordinal regression technique to model the responses from Likert items [@buerkner:2019:ORM]. Second, the two Likert items related to perceived professional autonomy can be modeled jointly using a multivariate regression model [^MULTIVARIATE]. 

[^MULTIVARIATE]: Please notice we are not talking about *multiple* regression regression.

As an exploratory analysis, we fitted a Bayesian model with these two modifications (see Online Appendix). And indeed the posterior predictive check of the model suggests that the modified model is better at pinning down the data-generating process of the Likert items than the unmodified model: the new model correctly pins down both the range, peaks and shape of the original data (Figure \@ref(fig:clplot)). The bottom line is the same: democratic performance of a country is still a strong predictor of journalists’ perceived professional autonomy in the modified model, although the magnitude of influence is not as great as shown by the unmodified model displayed in table 1.

```{r clplot, echo = FALSE, fig.cap = "Posterior predictive checks with 100 sets of simulated data based on the multivariate ordinal model"}
require(cowplot)
c9c10ppc <- readRDS("ppa_clmod_ppc.RDS")
c9c10ppc[[1]] + ggtitle(label = "", subtitle = "How much freedom do you personally have in selecting news stories you work on?") -> c9gg
c9c10ppc[[2]] + ggtitle(label = "", subtitle = "How much freedom do you personally have in deciding which aspects of a story should \n be emphasized?") -> c10gg
plot_grid(c9gg, c10gg, ncol = 1)
```

# Example 2: useNews

We used the useNews dataset [@puschmann:2020] to demonstrate how to perform a Bayesian multilevel regression analysis on a nonstochastic comparative dataset. The analysis was guided by a classic question in news value research: Does the distance between China and the host country of a media outlet increase the frequency of China coverage? In other words, we hypothesized that there is an ecological effect between a closer distance to China and an increased frequency of China coverage. Several distance measures were used to formulate our pre-registered hypotheses<!-- : physical distance between two capitals, trade volume between two countries, and cultural distance between two societies -->. Due to limited space, only the analysis of trade volume between two countries is displayed here. For other analyses, please refer to the online appendix.

For demonstrative purposes, our analysis was separated into two levels: outlet-level analysis (here) and article-level analysis (in the Online Appendix). The outlet-level analysis is an aggregated version of article-level analysis. It is used to demonstrate the flexibility of Bayesian analysis to handle two-level data with a very small data size. The article-level analysis is used to demonstrate the analysis of a massive three-level dataset.

## Data

The useNews is an openly available dataset [@puschmann:2020] that includes 2019 to 2020 media content data from an array of worldwide media outlets. The media content data was collected from the MediaCloud and made available as document-term matrices.

We used only the data from 2019 as the baseline<!-- . We --> and excluded media outlets that contributed <1,000 articles in that year. This threshold allowed us to retain media outlets that the frequency of China coverage can be reliably estimated. In total, <!-- 61 media outlets from Spain (n = 11), Romania (n = 11), US (n = 7), Austria (n = 6), Germany (n = 5), Norway (n = 5), Australia (n  = 4), Brazil (n = 4), UK (n = 4), South Korea (n = 2), and the Netherlands (n = 2) were included. These --> 61 media outlets contributed 1,525,871 articles.

In the outlet-level analysis, the data has 61 rows and each row represents a media outlet. The data contains a count of articles covering China ($z$), total number of articles ($n$), country of the outlet($k$), and distance measures ($x$).

## Coverage of China

A dictionary-based approach was used. The seed English and German dictionaries from the R package newsmap [@watanabe:2017:N] were used as the basis. The seed dictionaries contain words about Chinese, China, Beijing and Shanghai. We developed further the Spanish, Romanian, Korean, Portuguese, Norwegian, and Dutch dictionaries [^dict_val]. The dictionaries were applied to the 61 document-term matrices (format of the provided dataset) of all included media outlets. An article is classified as China coverage when at least one dictionary match is detected.

[^dict_val]: The validation of the dictionary is available in the online appendix.

<!-- ```{r, echo = FALSE} -->
<!-- Language <- c("English", "German", "Spanish", "Romanian", "Korean", "Portugese", "Norwegian", "Dutch") -->

<!-- Lexicon <- c("chinese, china, beijing, shanghai", "chinesisch*, chinesin*, chinese*, china*, peking, shanghai", "chino*, china*, pekin, shanghái", "chinez*, china, beijing, shanghai", "중국인, 중화, 중국, 베이징, 북경, 상하이, 상해", "chinês, chines*, china, pequim, xangai", "kineser*, kinesar*, kinesisk, kina, beijing, shanghai", "chinezen, chinees*, chinezin*, chines*, china, peking, beijing, sjanghai, shanghai") -->
<!-- papaja::apa_table(data.frame(Language, Lexicon), caption = "Dictionaries used for detecting China coverage") -->
<!-- ``` -->

## Distance measure: Trade volume

The volumes of import to and export from China with the host country of a media outlet was extracted from the 2019 edition of China Statistical Yearbook (http://www.stats.gov.cn/tjsj/ndsj/2019/indexeh.htm). This measure was log-transformed, because previous studies showed the relationship between news coverage and these distance measures is not linear [@grasland:2019:I;@wu:2000:SDI].
 
### Outlet-level analysis

We used negative binomial regression for this analysis [^EXT]. Media outlets are nested in countries <!-- (e.g. *Der Standard* and *Österreichischer Rundfunk* are nested in Austria) -->. Thus, a country-based variance is added ($\mu_{0k}$). We also added $\log{n_{j}}$ as an offset value. An offset value is a term that does not have the associated regression coefficient at the right-hand side of the regression equation. Effectively, we modeled the *rate* of China coverage. Applying the same notation under the introduction section, the multilevel regression equations are:

[^EXT]: We propose four extensions: hypothesis testing, investigating cross-level interaction, establishing informative prior, and testing for temporal changes (online appendix).

\begin{align}
\log{z_{jk}} &= \gamma_{00} + \mu_{0k} + \gamma_{01} \log{x_{k}} + \log{n_j} + \epsilon_{jk} \\
\log{z_{jk}} - \log{n_j} &= \gamma_{00} + \mu_{0k} + \gamma_{01} \log{x_{k}} +  \epsilon_{jk} \\
\log\frac{z_{jk}}{n_{j}} &= \gamma_{00} + \mu_{0k} + \gamma_{01} \log{x_{k}} + \epsilon_{jk}\end{align}

The estimand of interest, $\gamma_{01}$, is interpreted as the average unit change in the log rate of China coverage, $\log\frac{z_{jk}}{n_{j}}$, for each unit change in the log distance measure of the outlet $k$ with China.  

## Priors

There are four unknown parameters to be estimated: $\gamma_{00}$, $\mu_{0k}$, $\gamma_{01}$ and the negative binomial shape parameter $\phi$. Although @wu:2000:SDI is a possible reference point, we select not to use this as priors because China was not studied. <!-- Also, the modeling technique was quite different from the current one. --> In this analysis, we used weakly informative priors suggested by @lemoine:2019:M. The priors were the same as the ones used in Example 1. An additional gamma distribution of $\mathcal{Gamma}(0.01, 0.01)$ was used for the shape parameter $\phi$.

## Regression results

Table 2 shows a summary of all models from the outlet-level analysis using three different ways of modeling. Similar to Example 1, three methods give a similar point estimate but a wider interval estimate from the Bayesian model.

```{r}
import_reg <- readRDS("import_reg.RDS")
import_reg$term <- c("(Intercept)", "Log (Trade Volume)")
colnames(import_reg)[2:4] <- c("Bayesian", "MLE", "Disaggregation")
apa_table(import_reg, caption = "Point and interval estimates from outlet-level analysis using three different analytic strategies: Bayesian multilevel modeling, multilevel modeling using MLE, and Disaggregation approach")
```

### Posterior predictive checks

Similar to the previous example, we conducted the posterior predictive check (Figure  \@ref(fig:fig3x)). Our model can capture both the range, peak and shape of the original data. But there are many variations in the simulated data, probably due to the fact that the Bayesian model is based on just 61 data points and one single predictor.

```{r fig3x, echo = FALSE, fig.cap = "Posterior predictive checks with 100 sets of simulated data"}
pp_check(import_brms, ndraws = 100)
```

# Conclusion

In this paper, we argue the case for using Bayesian multilevel regression analysis to analyze data from comparative communication research. We demonstrate using the openly available useNews and WJS datasets that Bayesian analysis provides valid inference of ecological effects and can be done easily with the R package *brms*. We mainly used the WJS dataset to illustrate the strength of multilevel models, whereas the useNews dataset was selected as an example where Bayesian models are conceptually the better choice. However, both multilevel models and Bayesian analysis are connected approaches. 

First of all, both examples require a multilevel approach. We believe this is not an optional choice. The question then is why we should use a Bayesian approach as these models can also be estimated with a frequentist approach. As the useNews dataset represents a census-like situation, we argue a Bayesian approach is more appropriate. Of course, as the argument about the theoretical superpopulation shows, one could always justify a frequentist approach. However, there are also several practical reasons why a Bayesian approach is more suitable than a default approach.

First, the Bayesian interpretation of CIs is far more intuitive than the frequentist interpretation [@morey:2015]. Secondly, while all the models reported here can also be estimated within a frequentist framework, using a Bayesian framework and, more specifically, brms allows estimating them all with the same package. This also makes it easier to compare different competing models, as we have illustrated in our analysis. Third, comparing models and checking the predictive quality of the models is an essential part of a Bayesian framework. Of course, frequentist models can also be compared based on information criteria such as the BIC. However, PPCs that give by far the most detailed information about models and indicate in which area they potentially fail are only possible by using a Bayesian approach. Finally, while we discussed the question about priors as a potential challenge of Bayesian models, they are instead a strength of Bayesian models. Researchers must define priors to run a Bayesian regression analysis. As we could show, this limitation does not pose a controversial challenge as weakly informative priors can be chosen, and in most cases, enough data is available. Thus, the priors have almost no influence on the posterior distribution of the parameters. Moreover, if we have prior knowledge, it allows us to create even better models, as the first example has shown. 

Using a Bayesian approach still has several limitations. First, it takes more time to get the results of a Bayesian regression analysis. Estimating a Bayesian regression model is a computationally demanding task. It took four days on a regular computer to get the results of the Bayesian regression model with 1,525,871 articles (Example 2, article-level analysis). Still, eventually, it worked, and typical studies in comparative journalism research [e.g. 27,567 journalists in Example 1] can be estimated within a reasonable time frame. Secondly, if sampling procedures are used for a typical experimental design that could be replicated, using a frequentist approach is probably the less complicated approach and  a valid choice if combined with a preregistration of the hypotheses [@schaefer:2019:MES] [^PREREG]. In any other scenario, we believe the benefits of the Bayesian approach outweigh the potential limitations.

[^PREREG]: Even using the Bayesian approach, hypotheses should also be preregistered.

# Coda: A radical education reform proposal

@rinke:2018:PMA advocate a different teaching of statistics for communication research. In their opinion, the teaching of frequentist hypothesis testing would be less central. “Instead, more substantive concerns, replication, effect sizes, and better ways of drawing statistical inferences, including Bayesian methods, would take center stage.” [@rinke:2018:PMA, p. 18] We concur with their proposal and hope that this paper can convince the educators of our field to teach Bayesian methods to future communication researchers. Unlike what it was several years ago, we now have communication studies using the Bayesian approach published in top communication journals. Software and educational materials for doing Bayesian analysis have been tremendously improved. We see the availability of the R package *brms* [@burkner2017advanced] and approachable textbooks such as *Statistical Rethinking* [@mcelreath2020statistical] as watershed moments in the (re)mainstreamization of the Bayesian approach. Based on the recent development and the conclusion of this paper, we would like to make an even more radical education reform proposal than @rinke:2018:PMA’s: We should teach the Bayesian approach as the default way of drawing statistical inferences. The frequentist approach should instead be taught as an option specifically for the analysis of stochastic data from studies such as media experiments and simple random surveys. When it comes to teaching regression analysis, we should teach the statistical issues concerning the non-atomicity of the data, as well as the necessity of model checking with procedures such as posterior predictive checks.

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
