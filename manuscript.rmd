---
title             : "Bayesian (comparative) journalism studies"
shorttitle        : "BAYESIAN MULTILEVEL REGRESSION"

author: 
  - name          : "Chung-hong Chan"
    affiliation   : "1"
    corresponding : yes
    address       : "A5, 6 (section A), 68159 Mannheim, Germany"
    email         : "chung-hong.chan@mzes.uni-mannheim.de"
  - name          : "Adrian Rauschfleisch"
    affiliation   : "2"    

affiliation:
  - id            : "1"
    institution   : "Mannheimer Zentrum für Europäische Sozialforschung, Universität Mannheim, Germany"
  - id            : "2"
    institution   : "National Taiwan University, Taiwan"
    
authornote: |
  The authors would like to thank professor Kasper Welbers (Vrije Universiteit Amsterdam) for his advice on the development of the Dutch dictionary; Dr Junior Yuner Zhu (City University of Hong Kong) for sharing details about her study.  The authors report there are no competing interests to declare.
  Source code and data are available at (redacted). Prereg: https://osf.io/2h4w8/

abstract: |
  Comparative approaches such as comparative content analysis are frequently used in journalism studies. The purpose of this paper is to argue that researchers should refrain from using current (frequentist) analytical approaches to analyze data from a comparative study. It is because current approaches cannot account for non-atomicity (nested nature) and non-stochasticity (non-random sampling) of the data. Bayesian multilevel regression is suggested to be a ---and probably, the only--- valid analytical approach. Using the openly available *useNews* and *Worlds of Journalism Study* datasets and the R package *brms*, we demonstrate how to apply the Bayesian approach for the analysis of comparative data. We also address common queries of the approach such as choosing the priors and diagnosing the convergence of the model. Furthermore, we argue that choosing a Bayesian approach also solves the problem of often misinterpreted frequentist confidence intervals.
  
keywords          : "Bayesian inference, multilevel model, comparative communication research, ecological effect"
wordcount         : "8964"

bibliography      : ["manuscript.bib"]

floatsintext      : yes
figurelist        : no
tablelist         : no
figsintext        : yes
footnotelist      : no
linenumbers       : no
mask              : yes
draft             : no

documentclass     : "apa6"
classoption       : "man"
output:
 papaja::apa6_pdf:
   latex_engine: xelatex
---

```{r setup, include = FALSE}
library("papaja")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

<!-- *"The comparative approach attempts to reach conclusions beyond single cases and explains differences and similarities between objects of analysis against the backdrop of their contextual conditions."* @esser:2017:CRM -->

Communication research is often comparative, journalism studies are increasingly so [@hanusch:2019:C]. Comparative approach is used in modeling journalistic culture [e.g. @esser:2013:C], comparative news value research [e.g. @burggraaff:2017:T;@wilke:2012], news flow research [e.g. @wu:2000:SDI;@grasland:2019:I], among others. Around 40% of recent  comparative studies in journalism research are comparative content analysis [@hanusch:2019:C], which include news articles from various outlets and usually also from various countries. Then the included news articles are coded either manually or automatically. Another 10% of recent comparative studies in journalism research are surveys such as the Worlds of Journalism Study (WJS) [@hanusch:2017:CJC].

Although some comparative studies are purely descriptive, most of these studies are aimed to study how contextual conditions such as characteristics of media outlets (e.g. political orientation, online versus offline) or their countries (e.g. post-communist country) influence the content of news articles or behaviors of journalists. For example, some hypotheses are "the coverage of foreign countries by the news is primarily determined by geographical proximity and the status of the covered country" [@wilke:2012, p.306] and "the  degree of opinion-orientation will be highest in newspapers from Polarized Mediterranean systems and lowest in those from Anglo-American systems." [@esser:2013:C] Employing the language from epidemiology [@susser:1994], the effect on media content and journalistic behaviors in these hypotheses is assumed to be ecological. It assumes a macro contextual factor (e.g., newspapers from Polarized Mediterranean systems) is associated with an outcome at the micro-level (e.g., degree of opinion-orientation), namely, the article-level or journalist-level.

This study of ecological effects on individual behaviors has a long tradition in social sciences. A famous example was Émile Durkheim's investigation on the ecological factors associated with suicide [@durkheim1897suicide]. Durkheim found a higher suicide rate among citizens from Scandinavian countries than those from other European countries. Using today’s standard, Durkheim's analysis is descriptive and this descriptive approach for studying ecological effect is indeed still useful for hypothesis generation in comparative research. An example of hypothesis-generating comparative research is <!-- redacted for ijoc  -->AUTHOR1 <!-- @chan2020combining -->, in which the sentiment profiles of terrorism coverage from Muslim- and Christian-majority countries were visualized. Hypothesis-generating comparative study, however, is rare. Most studies have a confirmatory outlook. As we see from the example hypotheses above, most of these studies propose hypotheses to test for ecological effects. In this context usually traditional (frequentist) hypothesis testing approaches were used: @esser:2013:C lumped multiple outlets from the same country together and then used univariate analysis of variance to test for the differences in the proportion of opinion-orienting articles across countries. In @wu:2000:SDI, multiple stepwise regression was used. These approaches violate the underlying assumptions of the statistical analyses. It highlights the fact that comparative research introduces a feature that researchers usually overlook: media contents are clustered in a multilevel structure. A news article is nested within its media outlet and its media outlet is in turn nested within its country. Such data structure brings two problems: non-atomicity and non-stochasticity.

In our paper, we first discuss these two issues before we suggest Bayesian multilevel regression as a standard approach for comparative research in journalism studies. While Bayesian models might be slightly more complicated to estimate in contrast to frequentist models as priors have to be chosen, these models allow researchers to make "principled statements about precision and plausibility" [@morey:2015 p. 121] and solve the issue of misinterpreted confidence intervals [@hoekstra:2014:R;@rinke:2018:PMA]. Using the openly available *useNews* and WJS datasets and the R package *brms*, we then demonstrate how to apply a Bayesian approach for the analysis of comparative data.

## Non-atomicity

*"When it comes to regression, multilevel regression deserves to be the default approach"* @mcelreath2020statistical [p. 356]

It is perhaps well-known what ecological fallacy is. We commit ecological fallacy, when we make inference at the micro-level (e.g. Eating more chocolate makes one cleverer) based on macro-level data (e.g. the correlation between countries' chocolate consumption and countries' number of Nobel winners). The reverse of ecological fallacy, i.e. making inference at macro-level based on micro-level data, is equally fallacious. This fallacy is called atomistic fallacy [@gnaldi:2018:EFC]. 

In comparative research, it is easy to incorrectly assume that macro-level independent variables (e.g. Anglo-American systems) could be analyzed at the same level as the micro-level dependent variable (e.g. the degree of opinion-orientation). The manifestation of this incorrect assumption is to enter macro-level variables as independent variables in multiple regression analysis and regress them against a micro-level dependent variable. Suppose one wants to study a classic news value question of how the probability of covering China changes when the hosting country of the media outlet has a higher trade volume with China. It is related to the news value of "meaningfulness" in @galtung:1965:SFN. The relationship can be expressed in the following regression equation: let $y_{i}$ be the dependent variable at the micro-level (coverage of China or not) and $x_{i}$ be the independent variable at the macro-level (trade volume with China). Suppose there are $m$ news articles where $i = 1,2,\ldots,m$.

\begin{align}
  y_{i} = \beta_{0} + \beta_{1} x_{i} + \epsilon_{i}
\end{align}

This method is the so-called "disaggregation approach". From an epistemic standpoint, searching for ecological effect by studying the value of slope ($\beta_{1}$) leads to atomistic fallacy [@hox2017multilevel]. From a statistical standpoint, this approach violates the underlying independence assumption. Almost all frequentist statistical tests assume that the observations are independent of each other: $y_{i}$ given $x_{i}$ are independently identically distributed (i.i.d.). Articles from the same media outlet are hardly i.i.d.: they are subjected to being edited by the same editorial team, to being written by the same group of journalists exposed to the same journalistic culture and paid for by the same employer. Also, the way communication researchers collect their media content resembles cluster sampling rather than random sampling: one usually starts from a specific media outlet and then collects all relevant articles from it. The choice of media outlets or countries is almost always not randomly selected. <!-- [^1] --> This sampling approach makes the independence assumption even more fragile.

<!-- [^1]: If countries were randomly selected, we shall see more African countries in comparative works. It is because 1/4, or 54 out of 195, UN countries are in Africa. -->

Previous simulation studies have shown that ignoring this dependence can lead to false-positive associations [e.g. @clarke:2008:W;@chen:2012:IIL]. The problem is more severe when the dependent variable is a discrete binary variable [@clarke:2008:W], a common feature of content analysis. <!-- [^2] -->

<!-- [^2]: The non-atomicity argument has been in standard statistics textbooks for 40+ years and it might look repetitious. We believe it is still necessary, not least because multilevel modeling --unlike what @mcelreath2020statistical [p. 356] said-- is still not the default regression approach in communication research for hierarchical data and there are many justifications for using the disaggregation approach, e.g. an example in @zhu:2020:S [footnote 5]. <\!-- Even a multilevel modeling textbook advocates these justifications [@bickel2007multilevel]. -\-> See also @nezlek:2008:IMM for a criticism of these justifications.  -->

There are two solutions to this. The first solution is to aggregate the micro-level dependent variable across a macro-level independent variable. Suppose there are $n$ countries in the above example. One can aggregate the total number of articles covering China as $z_{k}$ for the $k$-th country, where $k=i,\ldots,n$. Then, we can do a count-based regression with a regression equation like so:

\begin{equation}
  \label{eq:2}
  \log{z_{k}} = \beta_{0} + \beta_{1} x_{k} + \epsilon_{k}
\end{equation}

Using this aggregation method, the unit of analysis effectively switches from article to country. This method is useful when $x$ is the only independent variable, akin @esser:2013:C. It is not technically committing the atomistic fallacy, when one uses the value of slope as the evidence for an ecological effect. However, this method still has important drawbacks. First, it cannot be used for data with more than two levels. In @esser:2013:C, for example, multiple media outlets from the same country were lumped together and in effect, assumed to be homogeneous and the information about media outlets was discarded. Second, this method discards a massive amount of information. It is better for analysis of a dependent micro-level variable with a reasonable aggregation function (e.g. counting a binary variable). For numerical variables, aggregation functions such as taking a mean or median cannot capture the spread of the micro-level variable [@bryk:1988:H]. Also, the effective sample size is reduced from the number of articles ($m$) to the number of countries ($n$). Nonetheless, this aggregation method, although inflexible, is still useful when the number of groups (e.g. $n$) is large. It is also useful to collapse micro-levels (e.g. article-level) that are not useful in answering one’s research questions.

Another solution is to use the multilevel model (linear mixed model, or hierarchical model). In a multilevel model, the effect on the micro-level dependent variable ($y$) is modeled with equations at different levels. Using the above example, $y_{ik}$ denotes whether the $i$-th article in the $j$-th country is covering China; $x_{k}$ denotes the trade volume with China of the $k$-th country where the media outlet of $i$-th article is located. 

\begin{align}
  y_{ik} &= \beta_{0} + \epsilon_{ik} \\
  \beta_{0} &= \gamma_{00} + \gamma_{01} x_{k} + \mu_{0k}
\end{align}

In these equations, $\gamma_{00}$ is the average slope, while $\mu_{0k}$ is group-dependent deviations of the slope from the average. It is usually set as having a normal distribution with a variance $\tau_{00}$, i.e. $\mu_{0k} \sim \mathcal{N}(0, \tau_{00})$. Instead of a single value, the regression coefficient $\gamma_{00}$ is assumed to be a distribution of values depending on a macro-level group. It addresses the problem of clustering of articles by macro-level variables. This model is called the varying-intercept model and is used frequently in social science research. We can then study the magnitude of $\gamma_{01}$ to determine the ecological effect.

The benefit of using multilevel modeling lies in its flexibility in handling multi-level data. Suppose we also want to consider the clustering of articles around $o$ different media outlets in the above example and $j$-th media outlet is the media outlet of the $i$-th article, where $j = 1,\ldots, o$. The multilevel regression equations are rewritten as:

\begin{align}
y_{ijk} &= \pi_{0} + \epsilon_{ijk} \\
\pi_{0} &= \beta_{00} + \mu_{0j} \\
\beta_{00} &= \gamma_{000} + \mu_{00k} + \gamma_{001} x_{k}
\end{align}

This flexibility is demonstrated in the study by @rinke:2016:ISB. He studied the likelihood of opinion justification in 1559 utterances nested in 329 news items, which were in turn nested in 101 news broadcasts. Multilevel logistic regression was used to model the natural three-level hierarchy of his data.

Multilevel modeling is the way to go for analyzing non-atomic data from comparative research. However, the conventional approach for statistical inference makes it not optimal. The conventional approach for statistical inference in multilevel modeling is maximum likelihood estimation (MLE). @stegmueller:2013:HMC demonstrates that MLE is associated with shrinkage (reduction of standard error, i.e. more false positives) and the shrinkage is more severe when the number of macro-level units (e.g. countries) is small. @stegmueller:2013:HMC proposes to use Bayesian analysis as a robust alternative [see counterarguments from @elff:2020:MAF;@bryan:2015:MMC]. Although less restrictive methods such as restricted maximum likelihood (REML) have been demonstrated to remediate the shrinkage issue of MLE [@elff:2020:MAF], we still agree with Stegmueller’s proposal for theoretical reasons. Our argument is more in line with @western:1994:BIC.

## Non-stochasticity

*"If Czech history **could** be repeated, we should of course find it desirable to test the other possibility each time and compare the results. Without such an experiment, all considerations of this kind remain a game of hypotheses."* [@kundera2020unbearable, emphasis added]

Before diving into our theoretical reasoning, it is important to revisit what frequentist inference, the current default but often misunderstood [@rinke:2018:PMA] mode of inference in communication science, is. Under the frequentist framework, each experiment is assumed to be one of infinite independent, **repeatable** experiments on randomly drawn samples from a population. Random experiments are assumed to be repeated arbitrarily often. Based on this assumption and with just one experiment from the current study, we make an estimation about the population. The discrepancy between the estimation from that one experiment and the actual value of the population is due to sampling error alone, i.e. which subjects were randomly sampled from the population. Randomized surveys, for example, are assumed to be repeatable through repeated random sampling of the population. Suppose we replicated the same survey for 100 times and we would obtain a slightly different sample every time. We then calculated the 90\% confidence interval of the mean for each of these 100 surveys. We should anticipate that more or less 90 out of these 100 confidence intervals would include the true mean of the population. We cannot say for sure exactly 90 out of these 100 confidence intervals would include the true mean of the population because repeated random sampling is indeed random and the process is **stochastic**. However, we can say 90 is more probable than 0 or 100.

Comparative content analytic studies in journalism studies, unlike randomized surveys or randomized media experiments, often collect all available data. In contrast to sampling, these studies are actually doing a census of all data. There is no way to get more data unless the scope of these studies is changed [@berk:1995:SIA]. It is especially true for modern large-N studies using automated content analytic techniques. @burggraaff:2017:T, for instance, "collected all available news items from a selection of major Dutch news outlets, both online and print" (p. 6) and that amounted to 762,095 articles from 9 outlets in the period of 2014--2015.

Census-style studies are not repeatable and thus they generate non-stochastic data. In other words, data from comparative journalism studies are fundamentally irrelevant for frequentist inference [@western:1994:BIC]. P-values and confidence intervals generated do not have the same meaning as those from repeatable studies. According to @western:1994:BIC [p. 413], these values from non-stochastic data "lack meaning even as abstract propositions".

A common counterargument to this is the classic one from @deming:1941:ICS [p. 45], who suggested that "[a]s a basis for scientific generalizations and decisions for action, a census is only a sample." This notion assumes a census of a population can be used to make inference on a theoretical device called *superpopulation* which "theoretically could exist, may have existed, or may exist in the future" [@gibbs:2015:IU, p.3]. In other words, a finite census-as-a-sample census is assumed to be a “representative sample” of an infinite superpopulation. This counterargument could be useful but unlike a regular sample, whose representativeness can be assessed, we can never assess the representativeness of the census with respect to the theoretical superpopulation. Echoing the quote at the beginning of this paragraph, there is no way to tell if the current Czech history is representative of all possible Czech histories in the multiple parallel universes.

Instead of invoking the theoretical device of superpopulation, we follow the arguments from @western:1994:BIC and @stegmueller:2013:HMC for comparative studies: Bayesian inference should be used for analyzing data from comparative research in our field. Choosing a Bayesian approach also solves the problem of misinterpretation of confidence intervals [@hoekstra:2014:R;@rinke:2018:PMA]. Switching to a Bayesian approach addresses this issue as Bayesian "credible intervals support an interpretation of probability in terms of plausibility" [@morey:2015, p.120]. <!-- However, for Bayesian models so-called priors have to be chosen by researchers. -->

Before we move on to the next section, it is important to point out that **non-stochastic data is not an essential condition for applying the Bayesian approach**. The same approach is equally applicable to both non-stochastic and stochastic comparative datasets. Therefore, one can analyze both randomized comparative surveys (such as WJS) and comparative content analytic studies with the same Bayesian analytic approach.

## Bayesian analysis

What is the probability for this paper being accepted by *IJOC*? Under the frequentist framework (and if *IJOC* were accepting papers stochastically), we can only find this out by repeatedly submitting this paper to *IJOC*, say for 100 times, and then count the frequency of acceptance in these repeated submissions. It is indeed impractical as well as inhumane to the editorial team of *IJOC*. Instead, we assert before submission that this paper has a probability of 24% for being accepted. That is the published acceptance rate of a similar journal. After this paper is submitted and is not desk rejected, the probability might be around 24% to 30%. After a month of waiting and our confidence is shaken a little, the probability might decrease to 10% to 20%. After the paper is favorably reviewed by two reviewers and an R&R is invited, the probability might increase to 40% to 60%. If you see this paper on *IJOC*'s website, then the probability is beyond doubt 100% (or 0% if we submit this paper again).

Without repeated experiment, these probabilities quantify our certainty on how likely this paper is being accepted, given the current available data . We revise our old beliefs (or **prior**, $p(\theta)$) with the new data ($X$) and form our revised belief (or **posterior**, $p(\theta|X)$). This can be summarized in the following equation [@gelman2020bayesian]:

\begin{align}
        P(\theta | X) \propto  P(\theta) P(X| \theta)
\end{align}
 
The $P(X| \theta)$ part is called likelihood function. In the *Journalism Studies* example, the likelihood function is based on rough rules from our experience and thus is not systematic. In actual analysis, we need to derive such a likelihood function based on the available data using methods such as Markov Chain Monte Carlo (MCMC). Nonetheless, the above equation indicates that there are only three ingredients in any Bayesian analysis: 1) data ($X$), 2) a method to derive the likelihood function $P(X | \theta)$ from the data, and 3) prior, $P(\theta)$. 

We skip elaborating part 1, because some, but certainty not all, researchers have the means to amass a massive amount. The mechanism of how part 2 can be used to derive the likelihood function is beyond the scope of this paper. <!-- We provide a tutorial in Appendix I(+++TODO) for those who want to get an intuition. --> The current standard method, MCMC, is much more computationally intensive and has only been possible on desktop computers since the mid-1990s [@robert:2011:SHM]. And only in the last few years that can be done efficiently [@hoffman2014no], thanks to the probabilistic programming language *Stan* [@gelman:2015:S]. R interfaces to Stan, such as *brms* [@burkner2017advanced], make Bayesian analysis even more accessible to R users. But still, it is important to be mindful that Bayesian analysis is much more computational intensive than methods such as MLE. Our benchmark suggests that Bayesian analysis needs at least 100 times more running time than MLE.

Part 3 (Prior) is arguably the most controversial part of Bayesian analysis. In the *Journalism Studies* example, we can select a reasonable prior (or **informative** prior) of 24% from published information. Finding previous studies for an informative prior is the logical first thing to do. @keating:2019:W show that one in every seven communication research papers published in major communication journals was a form of replication attempt. For these one seventh of communication research, there should be previous studies available to base one’s informative prior non-controversially.

For the other six sevenths, one might not have any information to set an informative prior. One option is to consult experts or to make an educated guess. Expert elicitation is a way to probe how the experts in the field think about the hypotheses. A standardized protocol for expert elicitation is available [@hanea:2017:IDE] and there are many software tools available to facilitate the process [^experttools].

[^experttools]: Some examples are the web-based MATCH Uncertainty Elicitation Tool (http://optics.eee.nottingham.ac.uk/match/uncertainty.php) and the R package SHELF.

But one person's expert opinion could be another person's wishful thinking. And this perceived subjectivity of specifying priors by experts’ judgment attracts wide-spread criticism from both statisticians [e.g. @efron1986isn] and social scientists [@elff:2020:MAF;@bryan:2015:MMC].

Undoubtedly, setting prior is consequential to the analysis [@van:2006:PB]. But the influence from priors is greatly weakened, when the data is getting bigger. Other than expert elicitation, another non-controversial way to specify priors --- at least in our opinion --- is to use  a weakly informative prior [@lemoine:2019:M]. In this way, one specifies only the possible range of the posterior<!-- [^4] -->. Some so-called "default weakly informative priors", e.g. $\mathcal{t}(1, 0, 0.25)$, have been suggested for typical regression models [@gelman:2008]. The documentation of Stan also provides information on how to choose an appropriate prior (https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations). 

<!-- [^4]: One could also use a completely *noninformative* prior such as $\mathcal{U}(-\infty, \infty)$. It is, however, no longer advised. -->

## Bayesian communication research

<!-- *"There is a general challenge to a prescription of more widespread use of Bayesian methods for multilevel modeling, and that is that such methods require statistical expertise beyond that of most applied social science researchers, as well as specialist software (or software with which such researchers are unfamiliar)."* @bryan:2015:MMC [p. 19] -->

Bayesian analysis is still a minority statistical method in social sciences. It is reasonable to say that Bayesian analysis is extremely rare in our field. Although some communication researchers adopted the method (e.g. AUTHOR2, AUTHOR3) <!-- [e.g. @kovic:2017:B;@chan2020high] -->, these studies are not comparative. As far as we know, the only available comparative research in journalism studies that Bayesian multilevel regression was used for studying ecological effect are @leeuw:2020:AAT and @heidenreich:2022:DE. We believe comparative communication research fits the use case of Bayesian multilevel regression analysis. Although there is another methodological introduction to Bayesian analysis aiming at communication researchers [@konijn:2015:PSP], one purpose of this paper is to demonstrate how to actually do the analysis using the R package *brms* [@burkner2017advanced] (see the source code in the Online Appendix https://osf.io/2h4w8/?view_only=be14dc637b8e4fd1a26ac5c64682b6d9 ). As the interface of *brms* is almost the same as *lme4* [@bates:2015:FLM, another R package for fitting multilevel models using MLE], *lme4* users might find *brms* extremely familiar. <!-- The above-quoted challenge about the prescription of more widespread use of Bayesian methods for multilevel modeling is no longer an issue.  -->

The two examples below show how the Bayesian multilevel regression analysis can be deployed to the common archetypes of comparative journalism studies. The first example is an analysis of a comparative, non-stochastic content analytic dataset. The second example is an analysis of a comparative, stochastic survey dataset.

# Example 1: useNews

We used the useNews dataset [@puschmann:2020] to demonstrate how to perform a typical Bayesian multilevel regression analysis on a nonstochastic comparative dataset. The analysis was guided by a classic question in news value research: Does the distance between China and the host country of a media outlet increase the frequency of China coverage? In other words, we studied the ecological effect of the distance from China on the frequency of China coverage. Several distance measures were used to formulate our pre-registered hypotheses: physical distance between two capitals, trade volume between two countries, and cultural distance between two societies.

For demonstrative purposes, our analysis was separated into two levels: outlet-level analysis and article-level analysis. The outlet-level analysis is an aggregated version of article-level analysis. It is used to demonstrate the flexibility of Bayesian analysis to handle two-level data with a very small data size. The article-level analysis is used to demonstrate the analysis of a massive three-level dataset.

## Data

The useNews is an openly available dataset [@puschmann:2020] that includes 2019 to 2020 media content data from an array of worldwide media outlets. The media content data was collected from the MediaCloud and made available as document-term matrices.

We used only the data from 2019 as the baseline. <!-- It is important to note that the Hong Kong protests, the 30th anniversary of the Tiananmen Square Protests and the 70th anniversary of the communist regime were in that year. Also, it was at the zenith of the US-China trade war.  -->

We excluded media outlets that contributed <1,000 articles in that year. This threshold allowed us to retain media outlets that the frequency of China coverage can be reliably estimated. In total, 61 media outlets from Spain (n = 11), Romania (n = 11), US (n = 7), Austria (n = 6), Germany (n = 5), Norway (n = 5), Australia (n  = 4), Brazil (n = 4), UK (n = 4), South Korea (n = 2), and the Netherlands (n = 2) were included. These 61 media outlets contributed 1,525,871 articles.

In the outlet-level analysis, the data has 61 rows and each row represents a media outlet. The data contains a count of articles covering China ($z$), total number of articles ($n$), country of the outlet($k$), and distance measures ($x$).

In the article-level analysis, the data has 1,525,871 rows and each row represents an article. The data contains a binary dependent variable of China coverage ($y$), media outlet($j$), country of the outlet($k$), and distance measures ($x$).

## Coverage of China

A dictionary-based approach was used. The seed English and German dictionaries from the R package newsmap [@watanabe:2017:N] were used as the basis. The seed dictionaries contain words about Chinese, China, Beijing and Shanghai. We developed further the Spanish, Romanian, Korean, Portuguese, Norwegian, and Dutch dictionaries [^dict_val]. The dictionaries were applied to the 61 document-term matrices (format of the provided dataset) of all included media outlets. An article is classified as China coverage when at least one dictionary match is detected.

[^dict_val]: The validation of the dictionary is available in the online appendix.

<!-- ```{r, echo = FALSE} -->
<!-- Language <- c("English", "German", "Spanish", "Romanian", "Korean", "Portugese", "Norwegian", "Dutch") -->

<!-- Lexicon <- c("chinese, china, beijing, shanghai", "chinesisch*, chinesin*, chinese*, china*, peking, shanghai", "chino*, china*, pekin, shanghái", "chinez*, china, beijing, shanghai", "중국인, 중화, 중국, 베이징, 북경, 상하이, 상해", "chinês, chines*, china, pequim, xangai", "kineser*, kinesar*, kinesisk, kina, beijing, shanghai", "chinezen, chinees*, chinezin*, chines*, china, peking, beijing, sjanghai, shanghai") -->
<!-- papaja::apa_table(data.frame(Language, Lexicon), caption = "Dictionaries used for detecting China coverage") -->
<!-- ``` -->

## Distance measures

Several distance measures were used as the independent variable to test our central research questions: physical distance, trade volume, and cultural distance. Physical distance is calculated as the Great-circle distance between Beijing and the capital of the hosting city of the media outlet (e.g. Bucharest). The volumes of import to and export from China with the host country of a media outlet was extracted from the 2019 edition of China Statistical Yearbook (http://www.stats.gov.cn/tjsj/ndsj/2019/indexeh.htm). Cultural distance from China to the host country of a media outlet was extracted from culturaldistance.com, which was calculated using the data from World Value Survey with the algorithm from @bell:2009:C. All of these distance measures were log-transformed, because previous studies showed the relationship between news coverage and these distance measures is not linear [@grasland:2019:I;@wu:2000:SDI].

## Modeling

For all analyses, we tried 3 different approaches: 1) Disaggregation approach (ignoring the multilevel structure); 2) Multilevel regression using MLE; and 3) Bayesian multilevel regression. It is important to remind our readers that the results from the three are likely to be similar, but it is not evidence for the disaggregation approach and the MLE approach is a replacement of the Bayesian approach [@morey:2015]. It is because the disaggregation approach commits atomistic fallacy and the MLE approach ignores the non-stochastic nature of the data.
 
### Outlet-level analysis

We used negative binomial regression for this analysis. Media outlets are nested in countries (e.g. *Der Standard* and *Österreichischer Rundfunk* are nested in Austria). Thus, a country-based variance is added ($\mu_{0k}$). We also added $\log{n_{j}}$ as an offset value. An offset value is a term that does not have the associated regression coefficient at the right-hand side of the regression equation \@ref(eq:2). Effectively, we modeled the *rate* of China coverage. Applying the same notation under the introduction section, the multilevel regression equations are:

\begin{align}
\log{z_{jk}} &= \gamma_{00} + \mu_{0k} + \gamma_{01} x_{k} + \log{n_j} + \epsilon_{jk} \\
\log{z_{jk}} - \log{n_j} &= \gamma_{00} + \mu_{0k} + \gamma_{01} x_{k} +  \epsilon_{jk} \\
\log\frac{z_{jk}}{n_{j}} &= \gamma_{00} + \mu_{0k} + \gamma_{01} x_{k} + \epsilon_{jk}\end{align}

The estimand of interest, $\gamma_{01}$, is interpreted as the average unit change in the log rate of China coverage, $\log\frac{z_{jk}}{n_{j}}$, for each unit change in the distance measure of the outlet $k$.  

In the disaggregation case, the multilevel structure is ignored and the regression equation becomes:

\begin{align}
\log\frac{z_{jk}}{n_{j}} &= \beta_{0} + \beta_{1} x_{k} + \epsilon_{j}
\end{align}

We named the models with different independent variables as model A1-4 respectively (A1: log import volume; A2: log export volume; A3: log physical distance; A4: log cultural distance).

### Article-level analysis

We used logistic regression for this analysis. However, due to the computational time needed to conduct the Bayesian regression in this case (it took 4 days on a regular computer), we only did the model with import volume as the independent variable.

In this analysis, an article is nested in its media outlet. The media outlet is in turn nested in its country. Two variance terms ($\mu_{0j}$ and $\mu_{00k}$) are needed. The multilevel regression equations are:

\begin{align}
p_{ijk} &= Pr(y_{ijk} = 1) \\
L_{ijk} &= \log{\frac{p_{ijk}}{1 - p_{ijk}}} \\
L_{ijk} &= \pi_{0} + \epsilon_{ijk} \\
\pi_{0} &= \beta_{00} + \mu_{0j} \\
\beta_{00} &= \gamma_{000} + \mu_{00k} + \gamma_{001} x_{k}
\end{align}

In the disaggregation case, the multilevel structure is ignored and the regression equations become:

\begin{align}
p_{ijk} &= Pr(y_{ijk} = 1) \\
L_{ijk} &= \log{\frac{p_{ijk}}{1 - p_{ijk}}} \\
L_{ijk} &= \beta_{0} + \beta_{1} x_{k} + \epsilon_{ijk}
\end{align}

We named this model as model B1.

## Priors

For the outlet-level analysis, there are four unknown parameters to be estimated: $\gamma_{00}$, $\mu_{0k}$, $\gamma_{01}$ and the negative binomial shape parameter $\phi$. For the article-level analysis, the four unknown parameters are $\gamma_{000}$, $\mu_{00k}$, $\gamma_{001}$ and $\mu_{0j}$. The idea of choosing priors is to select a probable probability distribution for each of these unknown parameters.

Usually, the first step for choosing priors is to review previous studies and look for possible values to be used as our informative priors. Although @wu:2000:SDI is a possible reference point, we select not to use this because China was not studied. Also, the modeling technique was quite different from the current one.

In this analysis, we used weakly informative priors suggested by @lemoine:2019:M.

For regression coefficients, e.g. $\gamma_{00}$, we used a normal distribution of $\mathcal{N}(0, 1)$. For variance terms, e.g. $\mu_{0k}$, a student t-distribution of $\mathcal{t}(3, 0, 2.5)$ was used. A gamma distribution of $\mathcal{Gamma}(0.01, 0.01)$ was used for the shape parameter $\phi$.

All, except the prior for regression coefficients, are default priors suggested by *brms*. 

### Other parameters

Other parameters for Bayesian multilevel regression analysis control how the MCMC should be performed. Three parameters are important: number of iterations, number of chains and *adapt_delta*. They are not some magic numbers that you can plug and play. However, it is in general safe to use the default values, i.e. number of iterations = 2000, number of chains = 4 and *adapt_delta* = 0.8, unless MCMC shows evidence of nonconvergence. *brms* is smart enough to suggest how to adjust these three values. Nonetheless, we provide a short guide on how to diagnose convergence.

## Regression results

Table \@ref(tab:t1) shows a summary of all models from the outlet-level analysis using three different ways of modeling. The Bayesian multilevel regression gave the *posterior distribution*, $P(\theta | X)$, of the regression coefficient ($\gamma_{001}$). It is a distribution and therefore we need ways to display both the central tendency and spread of the distribution. By default, *brms* displays the mean and the 95\% high density interval (HDI). [^5] These two values represent the point and interval estimates of the regression coefficient.

[^5]: HDI is one representation of the Bayesian credible interval. There should be no "correct" choice of the level of credibility but some established conventions. We choose, like many published works [e.g. @stegmueller:2013:HMC<!-- ; @kovic:2017:B; @chan2020high -->], the default value of 95\%. However, there is a general concern about this credibility level as being unstable for Bayesian analysis [@kruschke2014doing]. <!-- Some Bayesians, notably Andrew Gelman, report very liberal 50\% credible intervals. @mcelreath2020statistical selects 89\% to show the arbitrariness of the practice. @leeuw:2020:AAT select 90\%. -->

Consistently, point estimates are remarkably similar. Bayesian multilevel regression gave a wider 95% HDI than confidence intervals from frequentist methods. It is similar to previous studies [@stegmueller:2013:HMC; @elff:2020:MAF]. The results indicate that only a higher export or import volume with China and the host country of the media outlet is associated with more reporting of China. While the frequentist confidence intervals and the Bayesian credible interval seem to be similar, they are from a philosophical point of view totally different and lead to different forms of inference. The Bayesian approach allows us to say that *given the observed data, the true estimate of log import volume has 95% probability of falling within  0.18 and 0.33*. In contrast, the frequentist confidence interval only allows us to say that *when computing a confidence interval from the same type of data in repeated studies (if possible), 95% of the confidence intervals will include the true estimate of log import volume*. One of the misconceptions of frequentist confidence intervals is that they can be interpreted in a Bayesian way as described above [@morey:2015]. This example illustrates that a Bayesian approach allows us to interpret the results in a more intuitive way.

Table: (\#tab:t1) Point and interval estimates from outlet-level analysis using three different analytic strategies: Disaggregation approach, multilevel modeling using MLE and Bayesian multililevel modeling

| Model                           | Disaggregate ($\beta_{1}$, 95\% CI) | MLE ($\gamma_{01}$, 95\% CI) | Bayesian ($\gamma_{01}$, 95\% HDI) |
|:--------------------------------|:------------------------------------|------------------------------|------------------------------------|
| Model A1: Log import volume     | 0.25 (0.18, 0.33)                   | 0.25 (0.18, 0.33)            | 0.25 (0.18, 0.33)                  |
| Model A2: Log export volume     | 0.22 (0.14, 0.30)                   | 0.22 (0.13, 0.31)            | 0.22 (0.11, 0.33)                  |
| Model A3: Log Physical distance | -0.02 (-0.34, 0.24)                 | -0.10 (-0.55, 0.31)          | -0.12 (-0.63, 0.39)                |
| Model A4: Log Cultural distance | -0.45 (-1.26, 0.34)                 | -0.60 (-1.80, 0.55)          | -0.62 (-2.12, 0.78)                |

Model B1 confirms the above again (Table \@ref(tab:t2) ). Therefore, both article- and outlet-level analyses arrive at the same conclusion.

<!-- ```{r t2, results = "asis"} -->
<!-- ### TODO: to prim it further -->
<!-- require(brms) -->
<!-- require(parameters) -->
<!-- require(dplyr) -->
<!-- article_level <- readRDS("import_logit.RDS") -->
<!-- article_level %>% parameters(ci = 0.95, effects = "fixed", dispersion = TRUE, centrality = "mean", test = "pd") %>% as.data.frame -> res -->

<!-- res %>% dplyr::select(Parameter, Mean, SD, CI_low, CI_high) %>% rename(Estimate = Mean) %>% knitr::kable(format = "markdown", caption = "Bayesian multilevel logistic regression analyzing the article-level likelihood of China coverage in relation to log import volume of the outlet's country", digits = 2) -->
<!-- ``` -->

Table: (\#tab:t2) Bayesian multilevel logistic regression analyzing the article-level likelihood of China coverage in relation to log import volume of the outlet's country

| Parameter                          | Estimate | SD   | 95 \%HDI       |
|:-----------------------------------|:---------|------|----------------|
| $\gamma_{000}$ (Intercept)         | -7.06    | 0.81 | (-8.61, -5.44) |
| $\gamma_{001}$ (Log import volume) | 0.27     | 0.06 | (0.16, 0.38)   |

### Diagnosis of convergence

It is important to confirm the model is converged. We demonstrate how to confirm this with model A1.

The first step is Gelman-Rubin convergence diagnostic [@brooks:1998:GMM; @vehtari:2020:RNF]. *brms* displays this in the form of $\widehat{R}$. In general, $\widehat{R} \approx 1$. The model might not have converged, when $\widehat{R} \ge 1.01$ [@vehtari:2020:RNF]. The *summary()* function of brms displays the $\widehat{R}$ for each of the estimated unknown parameters. All parameter estimates have a $\widehat{R} = 1.0$.

The second step is the MCMC trace plot (can be obtained with the function *plot()*). The traceplot should display the so-called "fuzzy caterpillar" pattern (Figure \@ref(fig:fig1)). There should be no breakage.

```{r fig1, echo = FALSE, fig.cap = "The right panel in the MCMC trace plot from plot() function showing the fuzzy caterpillar pattern."}
require(brms)
import_brms <- readRDS("import_brms.RDS")
plot(import_brms)
```

An additional check is to study the autocorrelation of all chains. The correlogram should display low serial correlation (successive high correlation). The function *mcmc_plot* can be used to plot this (Figure \@ref(fig:fig2)).

If the diagnostics show evidence of nonconvergence, it is suggested to increase the number of iterations and number of chains. 

```{r fig2, echo = FALSE, fig.cap = "Correlogram of four chains showing no significant higher-order lag"}
mcmc_plot(import_brms, type = "acf")
```

### Further diagnostics

There are <!-- several --> two diagnostic checks that we can do to make sure our model is valid.

<!-- * Increasing number of iterations -->

<!-- The first test is to increase the number of iterations from 2000 to a higher value, e.g. 4000, and see if the result is consistent with the initial analysis. This check makes sure our result is not transient due to short iteration time. -->

<!-- Increasing the number of iterations did change a bit of the estimates, but only at the second decimal place. It shows that the results are consistent between the two analyses. -->

* Using another weakly informative prior

We can also use another weakly informative prior to see how our initial choice influences the result. We tried another weakly informative prior for the regression coefficients, $\mathcal{t}(1, 0, 0.25)$ [@gelman:2008]. Similarly, changing prior only impacts the second decimal place of the estimates.

* Posterior predictive checks

We can use posterior predictive checks to study how our model fits our data. The gist of the method is to use the fitted model to generate some simulated data ($y_{rep}$) and compare them with the original data ($y$). A good model should display a similar probability distribution of $y_{rep}$ and $y$. It can be done with the function *pp_check()* (Figure \@ref(fig:fig3) ).

```{r fig3, echo = FALSE, fig.cap = "Posterior predictive checks with 100 sets of simulated data"}
pp_check(import_brms, ndraws = 100)
```

## Extensions

We propose four extensions to the above barebone analysis: hypothesis testing, investigating cross-level interaction, establishing informative prior, and testing for temporal changes. The analyses in these extensions are not preregistered. Please refer to the online appendix for these extensions.

# Example 2: The Worlds of Journalism study

Example 1 above demonstrated how to conduct a Bayesian multilevel regression analysis. In this example, the same approach is applied to a stochastic dataset from a randomized survey. The starting point of this example is the recent study by @hamada:2021:DJA. Using the second wave data from WJS (2012-2016), the study seeks to study this hypothesis (the H2a in the paper): *The greater the level of democracy in a country, the more perceived professional autonomy journalists enjoy*.

It is important to point out that the study is actually a replication. The same hypothesis has been studied in an earlier study by @reich:2013:DJP, which uses the first wave data from WJS and has a remarkably similar title to that of @hamada:2021:DJA. The earlier study has studied this hypothesis (the H4): *Journalists’ perceived professional autonomy is positively associated with democratic performance and press freedom, and it is negatively related to political parallelism and state intervention.* The operationalizations of both professional autonomy (based on two questions in WJS) and level of democracy (based on the Economist Intelligence Unit's Index of Democracy) are the same in the two studies. 

Interestingly, the analytical approaches are also similar in the two studies. The study by @reich:2013:DJP contains multiple regression models on how journalist-level characters predict perceived professional autonomy. However, for country-level predictors such as Index of Democracy, @reich:2013:DJP apply an aggregated approach due to "methodological considerations" of "includ[ing] substantive country-level predictors in an OLS regression" (p. 146). Effectively, the analysis boils down to aggregating the perceived professional autonomy of all journalists into mean values according to baskets of Index of Democracy and then comparing those mean values by ANOVA. The subsequent study by @hamada:2021:DJA applies the same aggregated approach by studying the bivariate correlation between the mean perceived professional autonomy of all journalists in a country and the Index of Democracy of a country.

There are two weaknesses of the aggregated approach. First, a massive amount of information at the author-level was discarded and cannot leverage the hierarchical structure of the data. As shown in @reich:2013:DJP, author-level variables such as year of experience and being in a management position are also predictive of perceived professional autonomy. Taking the aggregated approach assumes that all of these author-level variables are homogeneous across countries. This assumption could create spurious correlation when the distribution of author-level variables is associated with contextual factors, e.g. Index of Democracy. Second, the two hypotheses specified in the two quoted studies refer to the perception at the micro-level ("the perceived professional autonomy journalists enjoy", "Journalists’ perceived professional autonomy"), not at the macro-level (e.g. the average level of perceived professional autonomy of journalists in a country). Interpreting the macro-level association wrongly at the micro-level leads to ecological fallacy.

## Bayesian analysis of WJS

Let's assume we were in the shoes of @hamada:2021:DJA and wanted to replicate the study by @reich:2013:DJP with the second wave WJS data. Bayesian multilevel regression is probably the only valid approach. First, it can take advantage of the hierarchical structure of the data and correctly estimate the contextual effect of democracy performance on the journalist-level perceived professional autonomy. Second, the incorporating of prior information from @reich:2013:DJP makes the intention to replicate clearer.

From the model 4 in @reich:2013:DJP, we know that the standardized beta coefficients for the variables rank and professional experience are .15 and .07 respectively. Unfortunately, the associated standard errors (or standard deviations) were not reported but only asterisks representing statistical significance (p < 0.001 for rank, p < 0.01 for professional experience). We estimated the maximum standard deviation based on the significance level. For example, the standard deviation of the standardized regression coefficient ($\sigma_{\beta'_{1}}$) of rank can be estimated by solving <!-- [^wald] -->:

\begin{align}
Pr(\frac{\beta'_{1}}{\sigma_{\beta'_{1}}}) &= \frac{0.001}{2} \sim \mathcal{N}(0, 1) \\
\frac{\beta'_{1}}{\sigma_{\beta'_{1}}} &= 3.29 \\
\frac{0.15}{\sigma_{\beta'_{1}}} &= 3.29 \\
\sigma_{\beta'_{1}} &= 0.045
\end{align}

The above information can be used as an informative prior of the current analysis. Unfortunately, the ANOVA result from @reich:2013:DJP cannot be used as the prior and we still need to use weakly informative prior. However, @reich:2013:DJP suggest that the relationship between Index of Democracy and perceived professional autonomy is in "J shape", with the average perceived professional autonomy of journalists in authoritarian regimes (the lowest end of democracy performance) being higher than those in hybrid regimes (3.92 vs 3.65). We can incorporate this prior information into our model by modeling a cubic relationship between Index of Democracy and perceived professional autonomy. Similar to the general procedure of conducting polynomial regression, we also create a parsimonious model assuming only a linear relationship to study whether the cubic regression improves the model fit. In other words, whether the relationship is really in "J shape" is studied [^jshape].

```{r, echo = FALSE}
pts_tab <- readRDS("pts_tab.RDS")
```

Another advantage of taking a Bayesian multilevel regression approach rather than the bivariate approach as in @reich:2013:DJP is the possibility to adjust for the possible confounding effects of other marco-level variables. For example, GDP per capita (according to the World Bank) is moderately correlated with the Index of Democracy (r = `r cor(pts_tab$eiu, pts_tab$gdppc, use = "complete")`). The correlation between Index of Democracy and perceived professional autonomy found by @reich:2013:DJP could be spurious and the GDP per capita were the actual determinant of perceived professional autonomy. We can adjust for the effect of GDP per capita by entering it also as an independent variable. The data can also provide such variance because there are countries with high GDP per capita but with low democracy (e.g. UAE and Qatar) and vice versa (e.g. Botswana and India).

In order to make the result from this replication comparable to that of @reich:2013:DJP, we also calculate the standardized regression coefficients, i.e. all variables are mean-centered before entering into the regression model. 

By looking at the regression coefficient and the 95\% HDI, the standardized regression coefficient for Index of Democracy is 0.24 (95\% HDI: 0.13 to 0.34). The Index of Democracy is not only a substantial predictor as 0 is not covered by the 95% HDI but also the strongest predictor among all other predictors (Rank: 0.15, Experience: 0.08, gender: -0.06, university degree: -0.01). It supports @hamada:2021:DJA's h2a, but our model does that without the problem of the aforementioned ecological fallacy and with adjustment for other potential confounders.

Regarding the possible "J shape" relationship suggested by @reich:2013:DJP, we compare the model fit ($R^{2}$) of the cubic model and the parsimonious model and the cubic model provides no improvement in model fit over the parsimonious model (both: 0.194) [^loo]. By looking at the conditional effects plot (Figure \@ref(fig:fig5)), it also suggests that the relationship is not "J shape". On average, a journalist in countries that scored a lower Index of Democracy has a lower perceived professional autonomy. 

```{r fig5, echo = FALSE, fig.cap = 'Conditional effects plot showing no "J shape" relationship between perceived professional autonomy and Index of Democracy.'}
require(ggplot2)
x <- readRDS("qmod_condit.RDS")
x$`eiu` + xlab("Index of Democracy (mean-centered)") + ylab("Perceived professional autonomy (mean-centered)") + theme_minimal() + scale_color_brewer(palette = "Dark2") + scale_fill_brewer(palette = "Dark2")
```

<!-- [^wald]: Here we used the z-approximation of Wald test rather than the more typical t-distribution-based Wald test because the degree of freedom (df) is not reported. And t-distribution approximates z-distribution asymptotically when df gets larger. -->

[^jshape]: Actually, the replication by @hamada:2021:DJA has shown that the relationship is not in such J shape. But we were assumed to be in the time point before such replication.

[^loo]: Another way to evaluate the model fit is to use leave-one-out cross validation. See the online appendix for the analysis.

# Conclusion

In this paper, we argue the case for using Bayesian multilevel regression analysis to analyze data from comparative communication research. We demonstrate using the openly available useNews and WJS datasets that Bayesian analysis provides valid inference of ecological effects and can be done easily with the R package *brms*. Using a Bayesian approach, however, has also clearly some disadvantages. Researchers have to define priors in order to run a Bayesian regression analysis. As we could show this limitation does not pose a controversial challenge as weakly informative priors can be chosen and in most cases enough data is available and thus the priors have almost no influence on the posterior distribution of the parameters. If we have prior knowledge it allows us to create even better models, as Example 2 has shown. Furthermore, it takes more time to get the results of a Bayesian regression analysis. Estimating a Bayesian regression model is a computationally demanding task. It took 4 days on a regular computer to get the results of the Bayesian regression model with 1,525,871 articles (Example 1, Model B1). Still, eventually it worked and typical studies in comparative journalism research [e.g. 6,525 articles in @esser:2013:C or 27,567 journalists in Example 2] can be estimated within a reasonable time frame [^TIME]. 

[^TIME]: We benchmarked the same model B1 but with subsets of data of some more "typical" sample sizes. A model with a large sample size of 30,000 can be estimated in less than 17 minutes on a typical 8-core computer. Although it is comparatively more computational intensive than *lme4*, the time needed is still reasonable.

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
